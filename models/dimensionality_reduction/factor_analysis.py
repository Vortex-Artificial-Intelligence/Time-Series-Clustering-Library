import torch
from torch import Tensor

import numpy as np
from numpy import ndarray
from typing import Union, Optional, Tuple
import warnings

from models.base import BaseDimensionalityReduction


class FactorAnalysis(BaseDimensionalityReduction):
    """
    Factor Analysis (FA)

    A statistical method used to describe variability among observed, correlated variables
    in terms of a potentially lower number of unobserved variables called factors.

    Parameters:
    -----------
    n_components : int
        Number of factors to extract
    max_iter : int, default=1000
        Maximum number of iterations for the EM algorithm
    tol : float, default=1e-8
        Tolerance for the convergence of the EM algorithm
    rotation : str, default=None
        Rotation method to apply to factors ('varimax', 'quartimax')
    cpu : bool, default=False
        If True, use CPU only
    device : int, default=0
        CUDA device index
    dtype : torch.dtype, default=torch.float64
        Data type for computations
    random_state : int, default=42
        Random seed for reproducibility

    Attributes:
    -----------
    components_ : Tensor of shape (n_components, n_features)
        Factor loadings matrix
    noise_variance_ : Tensor of shape (n_features,)
        Estimated noise variance for each feature
    log_likelihood_ : float
        Log-likelihood of the model
    n_iter_ : int
        Number of iterations run
    """

    def __init__(
        self,
        n_components: int,
        max_iter: int = 1000,
        tol: float = 1e-8,
        rotation: Optional[str] = None,
        cpu: Optional[bool] = False,
        device: Optional[int] = 0,
        dtype: Optional[torch.dtype] = torch.float64,
        random_state: Optional[int] = 42,
    ) -> None:
        super().__init__(
            n_components=n_components,
            cpu=cpu,
            device=device,
            dtype=dtype,
            random_state=random_state,
        )
        self.max_iter = max_iter
        self.tol = tol
        self.rotation = rotation
        self.noise_variance_ = None
        self.log_likelihood_ = None
        self.n_iter_ = 0

    def __str__(self) -> str:
        return "FactorAnalysis"

    def _expectation_step(
        self, X: Tensor, loadings: Tensor, psi: Tensor
    ) -> Tuple[Tensor, Tensor, Tensor]:
        """Expectation step of the EM algorithm"""
        n_samples, n_features = X.shape
        n_factors = loadings.shape[1]

        # Compute the covariance matrix
        sigma = loadings @ loadings.T + torch.diag(psi)

        try:
            sigma_inv = torch.inverse(sigma)
        except RuntimeError:
            # Add small regularization if matrix is singular
            sigma_reg = sigma + torch.eye(n_features, device=self.device) * 1e-6
            sigma_inv = torch.inverse(sigma_reg)

        # Compute the factor scores using the regression method
        beta = loadings.T @ sigma_inv  # Regression coefficients
        scores = X @ beta.T  # Factor scores

        # Compute the expected sufficient statistics
        xxT = X.T @ X
        xsT = X.T @ scores
        ssT = scores.T @ scores + n_samples * (
            torch.eye(n_factors, device=self.device) - beta @ loadings
        )

        return xsT, ssT, xxT

    def _maximization_step(
        self, xsT: Tensor, ssT: Tensor, xxT: Tensor, n_samples: int
    ) -> Tuple[Tensor, Tensor]:
        """Maximization step of the EM algorithm"""
        n_features, n_factors = xsT.shape

        # Update factor loadings
        try:
            loadings = xsT @ torch.inverse(ssT)
        except RuntimeError:
            # Add small regularization if matrix is singular
            ssT_reg = ssT + torch.eye(n_factors, device=self.device) * 1e-6
            loadings = xsT @ torch.inverse(ssT_reg)

        # Update noise variances
        psi = torch.diag(xxT - loadings @ xsT.T) / n_samples
        psi = torch.clamp(psi, min=1e-6)  # Ensure positive variance

        return loadings, psi

    def _varimax_rotation(
        self,
        loadings: Tensor,
        gamma: float = 1.0,
        max_iter: int = 100,
        tol: float = 1e-6,
    ) -> Tensor:
        """Apply varimax rotation to factor loadings"""
        n_features, n_factors = loadings.shape

        # Normalize the loadings matrix
        norm_factor = torch.sqrt(torch.sum(loadings**2, dim=1, keepdim=True))
        norm_factor = torch.clamp(norm_factor, min=1e-8)  # Avoid division by zero
        loadings_norm = loadings / norm_factor

        # Initialize rotation matrix
        R = torch.eye(n_factors, device=self.device, dtype=self.dtype)

        for i in range(max_iter):
            # Compute the rotated loadings
            loadings_rot = loadings_norm @ R

            # Compute the objective function
            lambda_sq = loadings_rot**2
            obj_prev = torch.sum(torch.var(lambda_sq, dim=0))

            # Update the rotation matrix
            for j in range(n_factors):
                for k in range(j + 1, n_factors):
                    # Compute the submatrix for factors j and k
                    u = loadings_norm[:, j]
                    v = loadings_norm[:, k]

                    # Compute the rotation angle
                    num = 2 * torch.sum(u * v * (u**2 - v**2) - gamma * u * v)
                    den = torch.sum(
                        (u**2 - v**2) ** 2 - 4 * u**2 * v**2 - gamma * (u**2 - v**2)
                    )
                    theta = 0.25 * torch.atan2(num, den)

                    # Create the rotation matrix for factors j and k
                    R_jk = torch.eye(n_factors, device=self.device, dtype=self.dtype)
                    R_jk[j, j] = torch.cos(theta)
                    R_jk[j, k] = -torch.sin(theta)
                    R_jk[k, j] = torch.sin(theta)
                    R_jk[k, k] = torch.cos(theta)

                    # Update the rotation matrix
                    R = R @ R_jk

            # Check convergence
            loadings_rot = loadings_norm @ R
            lambda_sq = loadings_rot**2
            obj = torch.sum(torch.var(lambda_sq, dim=0))

            if torch.abs(obj - obj_prev) < tol:
                break

        # Apply the rotation to the original loadings
        return loadings @ R

    def _quartimax_rotation(
        self,
        loadings: Tensor,
        max_iter: int = 100,
        tol: float = 1e-6,
    ) -> Tensor:
        """Apply quartimax rotation to factor loadings
        
        Quartimax rotation is an orthogonal rotation method that simplifies
        the columns of the factor loading matrix by maximizing the variance
        of squared loadings.
        """
        n_features, n_factors = loadings.shape

        # Normalize the loadings matrix
        norm_factor = torch.sqrt(torch.sum(loadings**2, dim=1, keepdim=True))
        norm_factor = torch.clamp(norm_factor, min=1e-8)  # Avoid division by zero
        loadings_norm = loadings / norm_factor

        R = torch.eye(n_factors, device=self.device, dtype=self.dtype)

        for i in range(max_iter):
            loadings_rot = loadings_norm @ R

            # Compute the objective function (quartimax criterion)
            lambda_sq = loadings_rot**2
            obj_prev = torch.sum(torch.var(lambda_sq))

            # Update the rotation matrix using a simplified approach
            # Quartimax can be seen as a special case of varimax with gamma=0
            for j in range(n_factors):
                for k in range(j + 1, n_factors):
                    # Compute the submatrix for factors j and k
                    u = loadings_norm[:, j]
                    v = loadings_norm[:, k]

                    # Compute the rotation angle for quartimax
                    # Quartimax maximizes the sum of fourth powers of loadings
                    num = 2 * torch.sum(u * v * (u**2 - v**2))
                    den = torch.sum((u**2 - v**2)**2 - 4 * u**2 * v**2)
                    theta = 0.25 * torch.atan2(num, den)

                    # Create the rotation matrix for factors j and k
                    R_jk = torch.eye(n_factors, device=self.device, dtype=self.dtype)
                    R_jk[j, j] = torch.cos(theta)
                    R_jk[j, k] = -torch.sin(theta)
                    R_jk[k, j] = torch.sin(theta)
                    R_jk[k, k] = torch.cos(theta)

                    # Update the rotation matrix
                    R = R @ R_jk

            # Check convergence
            loadings_rot = loadings_norm @ R
            lambda_sq = loadings_rot**2
            obj = torch.sum(torch.var(lambda_sq))

            if torch.abs(obj - obj_prev) < tol:
                break

        # Apply the rotation to the original loadings
        return loadings @ R

    def fit(self, X: Union[ndarray, Tensor]) -> "FactorAnalysis":
        """Fit the Factor Analysis model with X using the EM algorithm"""
        X = self.check_input(X)
        n_samples, n_features = X.shape

        self.mean_ = torch.mean(X, dim=0)
        X_centered = X - self.mean_

        # Initialize parameters
        loadings = (
            torch.randn(
                n_features, self.n_components, device=self.device, dtype=self.dtype
            )
            * 0.1
        )
        psi = torch.ones(n_features, device=self.device, dtype=self.dtype)

        # EM algorithm
        prev_log_likelihood = -float("inf")
        self.log_likelihood_ = -float("inf")

        for i in range(self.max_iter):
            try:
                # E-step
                xsT, ssT, xxT = self._expectation_step(X_centered, loadings, psi)

                # M-step
                loadings, psi = self._maximization_step(xsT, ssT, xxT, n_samples)

                # Compute log-likelihood
                sigma = loadings @ loadings.T + torch.diag(psi)

                try:
                    sigma_inv = torch.inverse(sigma)
                    log_det_sigma = torch.logdet(sigma)

                    # Log-likelihood: -n/2 * [log|sigma| + tr(sigma_inv * cov) + p*log(2π)]
                    cov = X_centered.T @ X_centered / n_samples
                    log_likelihood = (
                        -0.5
                        * n_samples
                        * (
                            log_det_sigma
                            + torch.trace(sigma_inv @ cov)
                            + n_features
                            * torch.log(torch.tensor(2 * np.pi, device=self.device))
                        )
                    )

                    # Check convergence
                    if torch.abs(log_likelihood - prev_log_likelihood) < self.tol:
                        self.log_likelihood_ = log_likelihood.item()
                        break

                    prev_log_likelihood = log_likelihood
                    self.log_likelihood_ = log_likelihood.item()

                except RuntimeError:
                    # Skip log-likelihood calculation if matrix inversion fails
                    warnings.warn(f"Log-likelihood calculation failed at iteration {i}")

            except RuntimeError as e:
                warnings.warn(f"EM algorithm failed at iteration {i}: {e}")
                break

            self.n_iter_ = i + 1

        # Apply rotation if specified
        if self.rotation == "varimax":
            loadings = self._varimax_rotation(loadings)
        elif self.rotation == "quartimax":
            loadings = self._quartimax_rotation(loadings)

        self.components_ = loadings.T  # Store as (n_components, n_features)
        self.noise_variance_ = psi

        return self

    def transform(self, X: Union[ndarray, Tensor]) -> Union[ndarray, Tensor]:
        """Transform X to the factor space"""
        if self.components_ is None:
            raise RuntimeError("FactorAnalysis must be fitted before transforming data")

        X = self.check_input(X)
        X_centered = X - self.mean_

        # Compute factor scores using regression method
        loadings = self.components_.T  # (n_features, n_components)

        # Compute the covariance matrix
        sigma = loadings @ loadings.T + torch.diag(self.noise_variance_)

        try:
            sigma_inv = torch.inverse(sigma)
        except RuntimeError:
            # Add small regularization if matrix is singular
            sigma_reg = sigma + torch.eye(sigma.shape[0], device=self.device) * 1e-6
            sigma_inv = torch.inverse(sigma_reg)

        # Compute regression coefficients
        beta = loadings.T @ sigma_inv  # (n_components, n_features)

        # Compute factor scores
        scores = X_centered @ beta.T  # (n_samples, n_components)

        return scores

    def get_covariance(self) -> Tensor:
        """Compute the covariance matrix of the fitted model"""
        if self.components_ is None:
            raise RuntimeError(
                "FactorAnalysis must be fitted before getting covariance"
            )

        loadings = self.components_.T  # (n_features, n_components)
        return loadings @ loadings.T + torch.diag(self.noise_variance_)


# Test
if __name__ == "__main__":
    torch.manual_seed(42)
    n_samples, n_features, n_factors = 1000, 100, 5

    true_loadings = torch.randn(n_features, n_factors, dtype=torch.float64)
    true_scores = torch.randn(n_samples, n_factors, dtype=torch.float64)
    noise = torch.randn(n_samples, n_features, dtype=torch.float64) * 0.1
    X = true_scores @ true_loadings.T + noise

    # Test FactorAnalysis with varimax rotation
    print("Testing FactorAnalysis with varimax rotation:")
    fa_varimax = FactorAnalysis(
        n_components=n_factors,
        max_iter=100,
        rotation="varimax"
    )
    X_transformed_varimax = fa_varimax.fit_transform(X)

    print("Original shape:", X.shape)
    print("Transformed shape:", X_transformed_varimax.shape)
    print("Log-likelihood:", fa_varimax.log_likelihood_)
    print("Number of iterations:", fa_varimax.n_iter_)
    print("Noise variance shape:", fa_varimax.noise_variance_.shape)
    print("Components shape:", fa_varimax.components_.shape)
    print()

    # Test FactorAnalysis with quartimax rotation
    print("Testing FactorAnalysis with quartimax rotation:")
    fa_quartimax = FactorAnalysis(
        n_components=n_factors,
        max_iter=100,
        rotation="quartimax"
    )
    X_transformed_quartimax = fa_quartimax.fit_transform(X)

    print("Original shape:", X.shape)
    print("Transformed shape:", X_transformed_quartimax.shape)
    print("Log-likelihood:", fa_quartimax.log_likelihood_)
    print("Number of iterations:", fa_quartimax.n_iter_)
    print("Noise variance shape:", fa_quartimax.noise_variance_.shape)
    print("Components shape:", fa_quartimax.components_.shape)
    print()

    # Test FactorAnalysis without rotation
    print("Testing FactorAnalysis without rotation:")
    fa_no_rotation = FactorAnalysis(
        n_components=n_factors,
        max_iter=100,
        rotation=None
    )
    X_transformed_no_rotation = fa_no_rotation.fit_transform(X)

    print("Original shape:", X.shape)
    print("Transformed shape:", X_transformed_no_rotation.shape)
    print("Log-likelihood:", fa_no_rotation.log_likelihood_)
    print("Number of iterations:", fa_no_rotation.n_iter_)
    print("Noise variance shape:", fa_no_rotation.noise_variance_.shape)
    print("Components shape:", fa_no_rotation.components_.shape)
